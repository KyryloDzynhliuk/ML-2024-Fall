> ## Постановка задачі
> 1. Ознайомитись з описом датасета, визначити цільову змінну (target) та основну задачу прогнозної моделі.
> 2. Виконати попередню обробку даних з метою подальшої побудови моделі.
> 3. Побудувати ансамблеву модель AdaBoost, взявши в якості базової моделі лінійну модель або DecisionTree. Підібрати гіперпараметр >n_estimators, learning_rate. Побудувати валідаційну криву, зробити висновок.

Отримала: 

![image](https://github.com/user-attachments/assets/f2ad1e07-790e-4427-a383-5f70996a7c24)

###  Validation Curve with AdaBoost

#### Аналіз розподілу та кривих:
1. Training Score (синя крива):
   - Точність на навчальній вибірці зростає зі збільшенням кількості естіматорів.
   - Крива стабільно зростає, не досягаючи насичення навіть при `500` естіматорах, що може вказувати на можливість перенавчання при значній кількості дерев.

2. Validation Score (помаранчева крива):
   - Точність на тестовій вибірці також зростає зі збільшенням кількості естіматорів, але після `100–200` естіматорів вона стабілізується.
   - Деякі коливання помітні після `200` естіматорів, але їх амплітуда незначна.

#### Оптимальні параметри:
- Найкраще співвідношення між навчанням та узагальненням досягається при кількості естіматорів `100–200`, коли тестова точність максимально близька до навчальної.

#### Точність:
- Максимальна точність на валідаційній вибірці сягає приблизно `0.908–0.910.`
- Надмірне збільшення кількості естіматорів після `200` не дає суттєвого приросту валідаційної точності.

---

> 4. Побудувати ансамблеву модель GradientBoosting, взявши в якості базової моделі неглибоке дерево рішень. Підібрати гіперпараметр n_estimators, learnign_rate. Побудувати валідаційну криву, зробити висновок.

Отримала: 

![image](https://github.com/user-attachments/assets/5897fccd-703f-4dec-bd9f-364e8ec84399)


### Validation Curve with GradientBoosting

#### Аналіз розподілу та кривих:
1. Training Score (синя крива):
   - Точність на навчальній вибірці стрімко зростає при невеликій кількості естіматорів (до `100`) і далі сповільнюється, наближаючись до насичення.
   - Крива стає майже горизонтальною після `300` естіматорів, що свідчить про досягнення максимальної ефективності.

2. Validation Score (помаранчева крива):
   - Точність на тестовій вибірці також швидко зростає до `100` естіматорів і стабілізується після цього.
   - Розрив між навчальною і валідаційною точністю менший порівняно з AdaBoost.

#### Оптимальні параметри:
- Кількість естіматорів `100–300` забезпечує баланс між навчанням та узагальненням.

#### Точність:
- Максимальна точність на валідаційній вибірці близько 0.910`0.910,`, що є порівнянним із AdaBoost.
- Валідаційна точність стабільніша, що свідчить про меншу чутливість до переобчислення.

---

### Загальні висновки:
1. AdaBoost:
   - Більш чутливий до збільшення кількості естіматорів.
   - Ризик перенавчання більший, оскільки навчальна точність продовжує зростати навіть при `500 `естіматорах, тоді як валідаційна точність насичується.

2. GradientBoosting:
   - Демонструє стабільніше узагальнення.
   - Менший ризик перенавчання, оскільки крива навчання досягає насичення раніше.

3. Рекомендація:
   - Для обох моделей оптимальна кількість естіматорів становить `100–200`. GradientBoosting є більш стабільним вибором у разі обмеження на складність моделі.

---

> 5. Зробити загальний висновок щодо усіх побудованих моделей у роботах 2-5. Яка модель вийшла найкращою та чому, на вашу думку?

На основі порівняння моделей в роботах` 2-5`, **GradientBoosting** виявляється кращою моделлю. Модель має меншу схильність до перенавчання завдяки більш поступовому додаванню нових дерев, що дозволяє ефективно виправляти помилки попередніх. Її стабільність на валідаційних даних свідчить про кращу узагальнюючу здатність, що робить її оптимальним вибором для даної задачі. Але її мінус в тому, що ця модель повільна.

В загальному можемо сказати: 

XGBoost: найкраща точність, підходить для складних даних.
LightGBM: оптимально для швидкого навчання.
GradientBoosting: стабільна, але повільна.
AdaBoost: простий, але менш точний.

---

> 6. (творче завдання) Побудувати модель, використовуючи XGBoost, LightGBM. Підібрати гіперпараметри, побудувати валідаційні криві. Зробити аналіз важливості ознак, побудувати графіки. Зробити висновки.

Отримано результати:


```
Найкращі параметри для XGBoost: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}
Точність на валідаційних даних (XGBoost): 0.9167223065250379
[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50
[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50
[LightGBM] [Info] Number of positive: 3705, number of negative: 29245
[LightGBM] [Info] Total Bins 751
[LightGBM] [Info] Number of data points in the train set: 32950, number of used features: 51
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.112443 -> initscore=-2.066025
[LightGBM] [Info] Start training from score -2.066025
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
Найкращі параметри для LightGBM: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}
Точність на валідаційних даних (LightGBM): 0.9166616084977239

```

![image](https://github.com/user-attachments/assets/0e1f3349-5e56-456d-9bf3-cc6c5cb447b0)
![image](https://github.com/user-attachments/assets/7f2ce7a3-e891-4d14-8802-d200f3c294ff)

**Аналіз валідаційної кривої з LightGBM:**

- Перенавчання:

  - Зі збільшенням кількості дерев точність на тренувальних даних постійно зростає, що свідчить про те, що модель починає запам'ятовувати шум у тренувальних даних, а не вивчати загальні закономірності.
  - Точність на валідаційних даних спочатку зростає, досягає максимуму, а потім починає стабілізуватися або навіть злегка знижуватися. Це класична ознака перенавчання.

- Оптимальна кількість дерев:

  - Оптимальна кількість дерев - це точка, в якій точність на валідаційних даних максимальна. На цьому графіку це приблизно 100-150 дерев.
  - Після цієї точки додавання нових дерев не покращує узагальнюючу здатність моделі і може навіть погіршити її.

- Загальна точність:

  - Максимальна точність на валідаційних даних досить висока, що свідчить про те, що модель загалом добре підходить для вирішення поставленого завдання.


**Аналіз валідаційної кривої з XGBoost:**

- Перенавчання:

  - Зі збільшенням кількості дерев точність на тренувальних даних постійно зростає, що свідчить про те, що модель починає запам'ятовувати шум у тренувальних даних, а не вивчати загальні закономірності. 
  - Точність на валідаційних даних спочатку зростає, досягає максимуму, а потім починає стабілізуватися або навіть злегка знижуватися. Це класична ознака перенавчання.

- Оптимальна кількість дерев:

  - Оптимальна кількість дерев - це точка, в якій точність на валідаційних даних максимальна. На цьому графіку це приблизно 100-150 дерев.
  - Після цієї точки додавання нових дерев не покращує узагальнюючу здатність моделі і може навіть погіршити її.

Загальна точність:

  - Максимальна точність на валідаційних даних досить висока (91.67%), що свідчить про те, що модель загалом добре підходить для вирішення поставленого завдання.

Висновок:

На основі представленого графіка можна зробити висновок, що модель XGBoost здатна досягти високої точності на даних, але при цьому схильна до перенавчання. 

Загальний висновок:

Для отримання оптимальних результатів необхідно ретельно підібрати гіперпараметри моделі і обрізати її, щоб уникнути перенавчання.


Топ-10 важливих ознак для   LightGBM і XGBoost відповідно: 

![image](https://github.com/user-attachments/assets/cdf5d62d-3c96-498b-b06f-11503bbf0175)

![image](https://github.com/user-attachments/assets/b7dd837d-d51f-4778-a24b-1c3b87d574f5)

